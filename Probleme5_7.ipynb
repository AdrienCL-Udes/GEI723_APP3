{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWTf7H60Z0i_"
   },
   "source": [
    "## Problème 5: Taille des lots de 512\n",
    "\n",
    "### Paramètre\n",
    "\n",
    "- Subsitution du gradient : Fonction de ReLu avec $\\alpha = 0.01$\n",
    "- Nombre de couches : 2\n",
    "- Nombre de neurones : 128\n",
    "- Nombre d’iterations : 4\n",
    "- Taux d’apprentissage : 0.01\n",
    "- Pas de discretisation $\\left(\\Delta T\\right)$ : 10 ms\n",
    "- **Taille des lots : 512**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neurons = 128\n",
    "nb_iteration = 4\n",
    "learning_rate = 0.01\n",
    "discretisation = 1\n",
    "lot_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpKksf2Ik4ga"
   },
   "source": [
    "#### Packages et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IJGOX5B8F7ic"
   },
   "outputs": [],
   "source": [
    "# pip install torch quantities sparse==0.11.0 > /dev/null\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, utils\n",
    "import torch\n",
    "import quantities as units\n",
    "from sparse import COO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BspwdQSOk8U-"
   },
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ngz0VkgVmJB6"
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Use the GPU unless there is none available.\n",
    "# If you don't have a CUDA enabled GPU, I recommned using Google Colab,\n",
    "# available at https://colab.research.google.com. Create a new notebook\n",
    "# and then go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
    "# Colab gives you access to up to 12 free continuous hours of a fairly recent GPU.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frOGRjz0mL_5"
   },
   "source": [
    "#### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hp-KevLKmLLP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdrienLaptop\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Let's download the MNIST dataset, available at https://www.openml.org/d/554\n",
    "# You can edit the argument data_home to the directory of your choice.\n",
    "# The dataset will be downloaded there; the default directory is ~/scikit_learn_data/\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True, data_home=None, as_frame=False)\n",
    "nb_of_samples, nb_of_features = X.shape\n",
    "# X = 70k samples, 28*28 features; y = 70k samples, 1 label (string)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X, y = utils.shuffle(X, y)\n",
    "\n",
    "# Convert the labels (string) to integers for convenience\n",
    "y = np.array(y, dtype=int)\n",
    "nb_of_ouputs = np.max(y) + 1\n",
    "\n",
    "# We'll normalize our input data in the range [0, 1[.\n",
    "X = X / pow(2, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgamMat1mXvu"
   },
   "source": [
    "#### Conversion en décharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "moU3ZUh8mSFG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And convert the data to a spike train using TTFS encoding\n",
    "dt = discretisation*units.ms\n",
    "duration_per_image = 100*units.ms\n",
    "absolute_duration = int(duration_per_image / dt)\n",
    "\n",
    "time_of_spike = (1 - X) * absolute_duration  # The brighter the pixel, the earlier the spike\n",
    "time_of_spike[X < .25] = 0  # \"Remove\" the spikes associated with darker pixels, which presumably carry less information\n",
    "\n",
    "sample_id, neuron_idx = np.nonzero(time_of_spike)\n",
    "\n",
    "# We use a sparse COO array to store the spikes for memory requirements\n",
    "# You can use the spike_train variable as if it were a tensor of shape (nb_of_samples, nb_of_features, absolute_duration)\n",
    "spike_train = COO((sample_id, neuron_idx, time_of_spike[sample_id, neuron_idx]),\n",
    "                  np.ones_like(sample_id), shape=(nb_of_samples, nb_of_features, absolute_duration))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAxIwqtumyda"
   },
   "source": [
    "#### Split entrainement/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9ffhq5RQm1Gg"
   },
   "outputs": [],
   "source": [
    "# Split in train/test\n",
    "nb_of_train_samples = int(nb_of_samples * 0.85)  # Keep 15% of the dataset for testing\n",
    "train_indices = np.arange(nb_of_train_samples)\n",
    "test_indices = np.arange(nb_of_train_samples, nb_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWlfjmhjmdPz"
   },
   "source": [
    "#### Création du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wqeJ9wNBm_84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0166, -0.0201, -0.0915,  ...,  0.1204, -0.0279,  0.1034],\n",
       "        [ 0.0904, -0.0630, -0.0854,  ...,  0.1121, -0.0862, -0.1139],\n",
       "        [ 0.0663, -0.0444,  0.0240,  ...,  0.0991, -0.0835,  0.0579],\n",
       "        ...,\n",
       "        [-0.2037,  0.0690,  0.0162,  ..., -0.0634,  0.0101, -0.0868],\n",
       "        [ 0.0544, -0.1185,  0.1433,  ..., -0.0279, -0.0658,  0.1325],\n",
       "        [ 0.0045,  0.0288,  0.0155,  ..., -0.0239, -0.0366, -0.0218]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a 2 layer network (1 hidden, 1 output)\n",
    "nb_hidden = nb_neurons  # Number of hidden neurons\n",
    "\n",
    "w1 = torch.empty((nb_of_features, nb_hidden), device=device, dtype=torch.float, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0., std=.1)\n",
    "\n",
    "w2 = torch.empty((nb_hidden, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True)\n",
    "torch.nn.init.normal_(w2, mean=0., std=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rrZ3qeWfnBwj"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cette class permet de calculer la sortie d'une fonction lors de la propagation avant et de personaliser la derivée lors de la retropropagation de l'erreur.\n",
    "Voir cet exemple pour plus de détails : https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\"\"\"\n",
    "class SpikeFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Dans la passe avant, nous recevons un tenseur contenant l'entrée (potential-threshold).\n",
    "    Nous appliquons la fonction Heaviside et renvoyons un tenseur contenant la sortie.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0 # On génère une décharge quand (potential-threshold) > 0\n",
    "        return out\n",
    "\n",
    "    \"\"\"\n",
    "    Dans la passe arrière, nous recevons un tenseur contenant le gradient de l'erreur par rapport à la sortie.\n",
    "    Nous calculons le gradient de l'erreur par rapport à l'entrée en utilisant la dérivée de la fonction ReLu.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_relu = torch.ones_like(input) # The derivativen ReLU function\n",
    "        grad_relu[input < 0] = 0.01        # Apply alpha = 0.01 to negative input\n",
    "        return grad_output.clone()*grad_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xAXZCMNsnIri"
   },
   "outputs": [],
   "source": [
    "def run_spiking_layer(input_spike_train, layer_weights, tau_v=20*units.ms, tau_i=5*units.ms, v_threshold=1.0):\n",
    "    \"\"\"Here we implement a current-LIF dynamic in PyTorch\"\"\"\n",
    "\n",
    "    # First, we multiply the input spike train by the weights of the current layer to get the current that will be added\n",
    "    # We can calculate this beforehand because the weights are constant in the forward pass (no plasticity)\n",
    "    input_current = torch.einsum(\"abc,bd->adc\", (input_spike_train, layer_weights))  # Equivalent to a matrix multiplication for tensors of dim > 2 using Einstein's Notation\n",
    "\n",
    "    recorded_spikes = []  # Array of the output spikes at each time t\n",
    "    membrane_potential_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "    membrane_current_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "\n",
    "    const_a = discretisation*units.ms / tau_i\n",
    "    alpha = np.exp(-(const_a.item()))\n",
    "\n",
    "    const_b = discretisation*units.ms / tau_v\n",
    "    beta = np.exp(-(const_b.item()))\n",
    "\n",
    "    for t in range(absolute_duration):  # For every timestep\n",
    "        # Apply the leak\n",
    "        membrane_potential_at_t = torch.mul(membrane_potential_at_t, beta) # Using tau_v with euler or exact method\n",
    "        membrane_current_at_t = torch.mul(membrane_current_at_t, alpha) # Using tau_i with euler or exact method\n",
    "\n",
    "        # Select the input current at time t\n",
    "        input_at_t = input_current[:, :, t]\n",
    "\n",
    "        # Integrate the input current\n",
    "        membrane_current_at_t += input_at_t\n",
    "\n",
    "        # Integrate the input to the membrane potential\n",
    "        membrane_potential_at_t += membrane_current_at_t\n",
    "\n",
    "        # Apply the non-differentiable function\n",
    "        recorded_spikes_at_t = SpikeFunction.apply(membrane_potential_at_t - v_threshold)\n",
    "        recorded_spikes.append(recorded_spikes_at_t)\n",
    "\n",
    "        # Reset the spiked neurons\n",
    "        membrane_potential_at_t[membrane_potential_at_t > v_threshold] = 0\n",
    "\n",
    "    recorded_spikes = torch.stack(recorded_spikes, dim=2) # Stack over time axis (Array -> Tensor)\n",
    "    return recorded_spikes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNEAhvjlnVqF"
   },
   "source": [
    "#### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dPyehEEZzc4x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 out of 116 in Epoch 1 -- loss : 971.6255\n",
      "Batch 2 out of 116 in Epoch 1 -- loss : 858.5232\n",
      "Batch 3 out of 116 in Epoch 1 -- loss : 665.7700\n",
      "Batch 4 out of 116 in Epoch 1 -- loss : 643.6924\n",
      "Batch 5 out of 116 in Epoch 1 -- loss : 572.7743\n",
      "Batch 6 out of 116 in Epoch 1 -- loss : 574.9306\n",
      "Batch 7 out of 116 in Epoch 1 -- loss : 521.8870\n",
      "Batch 8 out of 116 in Epoch 1 -- loss : 495.6405\n",
      "Batch 9 out of 116 in Epoch 1 -- loss : 460.9682\n",
      "Batch 10 out of 116 in Epoch 1 -- loss : 450.6552\n",
      "Batch 11 out of 116 in Epoch 1 -- loss : 441.1004\n",
      "Batch 12 out of 116 in Epoch 1 -- loss : 444.1115\n",
      "Batch 13 out of 116 in Epoch 1 -- loss : 438.7413\n",
      "Batch 14 out of 116 in Epoch 1 -- loss : 415.8080\n",
      "Batch 15 out of 116 in Epoch 1 -- loss : 407.0680\n",
      "Batch 16 out of 116 in Epoch 1 -- loss : 396.1100\n",
      "Batch 17 out of 116 in Epoch 1 -- loss : 379.3942\n",
      "Batch 18 out of 116 in Epoch 1 -- loss : 417.8709\n",
      "Batch 19 out of 116 in Epoch 1 -- loss : 385.1745\n",
      "Batch 20 out of 116 in Epoch 1 -- loss : 374.1435\n",
      "Batch 21 out of 116 in Epoch 1 -- loss : 382.8626\n",
      "Batch 22 out of 116 in Epoch 1 -- loss : 384.4743\n",
      "Batch 23 out of 116 in Epoch 1 -- loss : 392.7113\n",
      "Batch 24 out of 116 in Epoch 1 -- loss : 367.8708\n",
      "Batch 25 out of 116 in Epoch 1 -- loss : 355.7210\n",
      "Batch 26 out of 116 in Epoch 1 -- loss : 365.5179\n",
      "Batch 27 out of 116 in Epoch 1 -- loss : 353.8137\n",
      "Batch 28 out of 116 in Epoch 1 -- loss : 331.9696\n",
      "Batch 29 out of 116 in Epoch 1 -- loss : 336.1277\n",
      "Batch 30 out of 116 in Epoch 1 -- loss : 335.6478\n",
      "Batch 31 out of 116 in Epoch 1 -- loss : 346.1602\n",
      "Batch 32 out of 116 in Epoch 1 -- loss : 369.9493\n",
      "Batch 33 out of 116 in Epoch 1 -- loss : 334.7577\n",
      "Batch 34 out of 116 in Epoch 1 -- loss : 344.8550\n",
      "Batch 35 out of 116 in Epoch 1 -- loss : 338.2172\n",
      "Batch 36 out of 116 in Epoch 1 -- loss : 347.8431\n",
      "Batch 37 out of 116 in Epoch 1 -- loss : 333.9823\n",
      "Batch 38 out of 116 in Epoch 1 -- loss : 305.6435\n",
      "Batch 39 out of 116 in Epoch 1 -- loss : 325.2735\n",
      "Batch 40 out of 116 in Epoch 1 -- loss : 316.2612\n",
      "Batch 41 out of 116 in Epoch 1 -- loss : 316.2928\n",
      "Batch 42 out of 116 in Epoch 1 -- loss : 351.5357\n",
      "Batch 43 out of 116 in Epoch 1 -- loss : 332.8704\n",
      "Batch 44 out of 116 in Epoch 1 -- loss : 328.9944\n",
      "Batch 45 out of 116 in Epoch 1 -- loss : 329.4421\n",
      "Batch 46 out of 116 in Epoch 1 -- loss : 317.4963\n",
      "Batch 47 out of 116 in Epoch 1 -- loss : 326.3614\n",
      "Batch 48 out of 116 in Epoch 1 -- loss : 319.6672\n",
      "Batch 49 out of 116 in Epoch 1 -- loss : 300.6314\n",
      "Batch 50 out of 116 in Epoch 1 -- loss : 315.2914\n",
      "Batch 51 out of 116 in Epoch 1 -- loss : 301.3840\n",
      "Batch 52 out of 116 in Epoch 1 -- loss : 324.6230\n",
      "Batch 53 out of 116 in Epoch 1 -- loss : 336.8573\n",
      "Batch 54 out of 116 in Epoch 1 -- loss : 322.1491\n",
      "Batch 55 out of 116 in Epoch 1 -- loss : 314.6559\n",
      "Batch 56 out of 116 in Epoch 1 -- loss : 320.5628\n",
      "Batch 57 out of 116 in Epoch 1 -- loss : 315.3636\n",
      "Batch 58 out of 116 in Epoch 1 -- loss : 293.8680\n",
      "Batch 59 out of 116 in Epoch 1 -- loss : 302.9858\n",
      "Batch 60 out of 116 in Epoch 1 -- loss : 307.9171\n",
      "Batch 61 out of 116 in Epoch 1 -- loss : 322.7571\n",
      "Batch 62 out of 116 in Epoch 1 -- loss : 316.6963\n",
      "Batch 63 out of 116 in Epoch 1 -- loss : 294.4622\n",
      "Batch 64 out of 116 in Epoch 1 -- loss : 320.3002\n",
      "Batch 65 out of 116 in Epoch 1 -- loss : 323.2456\n",
      "Batch 66 out of 116 in Epoch 1 -- loss : 304.4483\n",
      "Batch 67 out of 116 in Epoch 1 -- loss : 309.4008\n",
      "Batch 68 out of 116 in Epoch 1 -- loss : 282.1600\n",
      "Batch 69 out of 116 in Epoch 1 -- loss : 304.0454\n",
      "Batch 70 out of 116 in Epoch 1 -- loss : 290.2509\n",
      "Batch 71 out of 116 in Epoch 1 -- loss : 305.2626\n",
      "Batch 72 out of 116 in Epoch 1 -- loss : 293.1819\n",
      "Batch 73 out of 116 in Epoch 1 -- loss : 292.7248\n",
      "Batch 74 out of 116 in Epoch 1 -- loss : 278.2464\n",
      "Batch 75 out of 116 in Epoch 1 -- loss : 304.8099\n",
      "Batch 76 out of 116 in Epoch 1 -- loss : 305.5799\n",
      "Batch 77 out of 116 in Epoch 1 -- loss : 284.2729\n",
      "Batch 78 out of 116 in Epoch 1 -- loss : 285.7244\n",
      "Batch 79 out of 116 in Epoch 1 -- loss : 298.4602\n",
      "Batch 80 out of 116 in Epoch 1 -- loss : 297.6826\n",
      "Batch 81 out of 116 in Epoch 1 -- loss : 312.3485\n",
      "Batch 82 out of 116 in Epoch 1 -- loss : 289.9891\n",
      "Batch 83 out of 116 in Epoch 1 -- loss : 284.2990\n",
      "Batch 84 out of 116 in Epoch 1 -- loss : 303.4990\n",
      "Batch 85 out of 116 in Epoch 1 -- loss : 290.2304\n",
      "Batch 86 out of 116 in Epoch 1 -- loss : 297.5834\n",
      "Batch 87 out of 116 in Epoch 1 -- loss : 282.7314\n",
      "Batch 88 out of 116 in Epoch 1 -- loss : 272.2634\n",
      "Batch 89 out of 116 in Epoch 1 -- loss : 306.3692\n",
      "Batch 90 out of 116 in Epoch 1 -- loss : 294.7213\n",
      "Batch 91 out of 116 in Epoch 1 -- loss : 258.1651\n",
      "Batch 92 out of 116 in Epoch 1 -- loss : 252.9017\n",
      "Batch 93 out of 116 in Epoch 1 -- loss : 256.8152\n",
      "Batch 94 out of 116 in Epoch 1 -- loss : 296.2152\n",
      "Batch 95 out of 116 in Epoch 1 -- loss : 282.5955\n",
      "Batch 96 out of 116 in Epoch 1 -- loss : 278.5988\n",
      "Batch 97 out of 116 in Epoch 1 -- loss : 272.4971\n",
      "Batch 98 out of 116 in Epoch 1 -- loss : 291.4546\n",
      "Batch 99 out of 116 in Epoch 1 -- loss : 268.5172\n",
      "Batch 100 out of 116 in Epoch 1 -- loss : 286.7435\n",
      "Batch 101 out of 116 in Epoch 1 -- loss : 300.4055\n",
      "Batch 102 out of 116 in Epoch 1 -- loss : 284.5961\n",
      "Batch 103 out of 116 in Epoch 1 -- loss : 265.5727\n",
      "Batch 104 out of 116 in Epoch 1 -- loss : 265.4868\n",
      "Batch 105 out of 116 in Epoch 1 -- loss : 299.3651\n",
      "Batch 106 out of 116 in Epoch 1 -- loss : 278.5961\n",
      "Batch 107 out of 116 in Epoch 1 -- loss : 296.0327\n",
      "Batch 108 out of 116 in Epoch 1 -- loss : 267.9901\n",
      "Batch 109 out of 116 in Epoch 1 -- loss : 287.0102\n",
      "Batch 110 out of 116 in Epoch 1 -- loss : 297.1576\n",
      "Batch 111 out of 116 in Epoch 1 -- loss : 268.6131\n",
      "Batch 112 out of 116 in Epoch 1 -- loss : 261.8525\n",
      "Batch 113 out of 116 in Epoch 1 -- loss : 274.2104\n",
      "Batch 114 out of 116 in Epoch 1 -- loss : 283.5545\n",
      "Batch 115 out of 116 in Epoch 1 -- loss : 320.8973\n",
      "Batch 116 out of 116 in Epoch 1 -- loss : 262.5549\n",
      "Epoch 1 -- loss : 343.7133\n",
      "Batch 1 out of 116 in Epoch 2 -- loss : 284.8977\n",
      "Batch 2 out of 116 in Epoch 2 -- loss : 289.2614\n",
      "Batch 3 out of 116 in Epoch 2 -- loss : 241.9446\n",
      "Batch 4 out of 116 in Epoch 2 -- loss : 266.1585\n",
      "Batch 5 out of 116 in Epoch 2 -- loss : 275.8737\n",
      "Batch 6 out of 116 in Epoch 2 -- loss : 323.7306\n",
      "Batch 7 out of 116 in Epoch 2 -- loss : 281.1374\n",
      "Batch 8 out of 116 in Epoch 2 -- loss : 301.1686\n",
      "Batch 9 out of 116 in Epoch 2 -- loss : 283.5832\n",
      "Batch 10 out of 116 in Epoch 2 -- loss : 278.9318\n",
      "Batch 11 out of 116 in Epoch 2 -- loss : 266.8281\n",
      "Batch 12 out of 116 in Epoch 2 -- loss : 268.6858\n",
      "Batch 13 out of 116 in Epoch 2 -- loss : 264.3856\n",
      "Batch 14 out of 116 in Epoch 2 -- loss : 280.8573\n",
      "Batch 15 out of 116 in Epoch 2 -- loss : 290.3727\n",
      "Batch 16 out of 116 in Epoch 2 -- loss : 277.4762\n",
      "Batch 17 out of 116 in Epoch 2 -- loss : 267.7854\n",
      "Batch 18 out of 116 in Epoch 2 -- loss : 312.8446\n",
      "Batch 19 out of 116 in Epoch 2 -- loss : 270.4979\n",
      "Batch 20 out of 116 in Epoch 2 -- loss : 288.8146\n",
      "Batch 21 out of 116 in Epoch 2 -- loss : 272.5372\n",
      "Batch 22 out of 116 in Epoch 2 -- loss : 291.2635\n",
      "Batch 23 out of 116 in Epoch 2 -- loss : 291.3405\n",
      "Batch 24 out of 116 in Epoch 2 -- loss : 260.6786\n",
      "Batch 25 out of 116 in Epoch 2 -- loss : 256.0583\n",
      "Batch 26 out of 116 in Epoch 2 -- loss : 282.9938\n",
      "Batch 27 out of 116 in Epoch 2 -- loss : 263.8809\n",
      "Batch 28 out of 116 in Epoch 2 -- loss : 243.9893\n",
      "Batch 29 out of 116 in Epoch 2 -- loss : 255.9671\n",
      "Batch 30 out of 116 in Epoch 2 -- loss : 249.4600\n",
      "Batch 31 out of 116 in Epoch 2 -- loss : 281.2520\n",
      "Batch 32 out of 116 in Epoch 2 -- loss : 288.5119\n",
      "Batch 33 out of 116 in Epoch 2 -- loss : 258.8684\n",
      "Batch 34 out of 116 in Epoch 2 -- loss : 270.3339\n",
      "Batch 35 out of 116 in Epoch 2 -- loss : 258.3821\n",
      "Batch 36 out of 116 in Epoch 2 -- loss : 278.3881\n",
      "Batch 37 out of 116 in Epoch 2 -- loss : 261.1696\n",
      "Batch 38 out of 116 in Epoch 2 -- loss : 246.2645\n",
      "Batch 39 out of 116 in Epoch 2 -- loss : 252.8259\n",
      "Batch 40 out of 116 in Epoch 2 -- loss : 239.6899\n",
      "Batch 41 out of 116 in Epoch 2 -- loss : 251.8314\n",
      "Batch 42 out of 116 in Epoch 2 -- loss : 279.2661\n",
      "Batch 43 out of 116 in Epoch 2 -- loss : 260.5682\n",
      "Batch 44 out of 116 in Epoch 2 -- loss : 276.6495\n",
      "Batch 45 out of 116 in Epoch 2 -- loss : 266.3528\n",
      "Batch 46 out of 116 in Epoch 2 -- loss : 263.9809\n",
      "Batch 47 out of 116 in Epoch 2 -- loss : 264.6602\n",
      "Batch 48 out of 116 in Epoch 2 -- loss : 259.7257\n",
      "Batch 49 out of 116 in Epoch 2 -- loss : 251.8869\n",
      "Batch 50 out of 116 in Epoch 2 -- loss : 255.6168\n",
      "Batch 51 out of 116 in Epoch 2 -- loss : 237.4002\n",
      "Batch 52 out of 116 in Epoch 2 -- loss : 262.3739\n",
      "Batch 53 out of 116 in Epoch 2 -- loss : 276.9152\n",
      "Batch 54 out of 116 in Epoch 2 -- loss : 259.4162\n",
      "Batch 55 out of 116 in Epoch 2 -- loss : 266.4335\n",
      "Batch 56 out of 116 in Epoch 2 -- loss : 277.7852\n",
      "Batch 57 out of 116 in Epoch 2 -- loss : 280.4910\n",
      "Batch 58 out of 116 in Epoch 2 -- loss : 261.4300\n",
      "Batch 59 out of 116 in Epoch 2 -- loss : 263.4895\n",
      "Batch 60 out of 116 in Epoch 2 -- loss : 268.1577\n",
      "Batch 61 out of 116 in Epoch 2 -- loss : 271.1516\n",
      "Batch 62 out of 116 in Epoch 2 -- loss : 262.1717\n",
      "Batch 63 out of 116 in Epoch 2 -- loss : 247.6587\n",
      "Batch 64 out of 116 in Epoch 2 -- loss : 263.7312\n",
      "Batch 65 out of 116 in Epoch 2 -- loss : 272.9365\n",
      "Batch 66 out of 116 in Epoch 2 -- loss : 261.5815\n",
      "Batch 67 out of 116 in Epoch 2 -- loss : 265.8575\n",
      "Batch 68 out of 116 in Epoch 2 -- loss : 253.2440\n",
      "Batch 69 out of 116 in Epoch 2 -- loss : 266.4575\n",
      "Batch 70 out of 116 in Epoch 2 -- loss : 253.0571\n",
      "Batch 71 out of 116 in Epoch 2 -- loss : 272.4289\n",
      "Batch 72 out of 116 in Epoch 2 -- loss : 260.7462\n",
      "Batch 73 out of 116 in Epoch 2 -- loss : 257.1212\n",
      "Batch 74 out of 116 in Epoch 2 -- loss : 241.2809\n",
      "Batch 75 out of 116 in Epoch 2 -- loss : 258.8224\n",
      "Batch 76 out of 116 in Epoch 2 -- loss : 267.5813\n",
      "Batch 77 out of 116 in Epoch 2 -- loss : 243.9495\n",
      "Batch 78 out of 116 in Epoch 2 -- loss : 250.7791\n",
      "Batch 79 out of 116 in Epoch 2 -- loss : 255.9731\n",
      "Batch 80 out of 116 in Epoch 2 -- loss : 271.8600\n",
      "Batch 81 out of 116 in Epoch 2 -- loss : 264.8401\n",
      "Batch 82 out of 116 in Epoch 2 -- loss : 264.1916\n",
      "Batch 83 out of 116 in Epoch 2 -- loss : 262.1534\n",
      "Batch 84 out of 116 in Epoch 2 -- loss : 270.7998\n",
      "Batch 85 out of 116 in Epoch 2 -- loss : 266.9587\n",
      "Batch 86 out of 116 in Epoch 2 -- loss : 272.4027\n",
      "Batch 87 out of 116 in Epoch 2 -- loss : 257.8622\n",
      "Batch 88 out of 116 in Epoch 2 -- loss : 247.1152\n",
      "Batch 89 out of 116 in Epoch 2 -- loss : 275.5441\n",
      "Batch 90 out of 116 in Epoch 2 -- loss : 267.4066\n",
      "Batch 91 out of 116 in Epoch 2 -- loss : 232.2152\n",
      "Batch 92 out of 116 in Epoch 2 -- loss : 231.4193\n",
      "Batch 93 out of 116 in Epoch 2 -- loss : 228.5131\n",
      "Batch 94 out of 116 in Epoch 2 -- loss : 260.3548\n",
      "Batch 95 out of 116 in Epoch 2 -- loss : 250.2867\n",
      "Batch 96 out of 116 in Epoch 2 -- loss : 243.2119\n",
      "Batch 97 out of 116 in Epoch 2 -- loss : 238.3994\n",
      "Batch 98 out of 116 in Epoch 2 -- loss : 253.8121\n",
      "Batch 99 out of 116 in Epoch 2 -- loss : 235.3807\n",
      "Batch 100 out of 116 in Epoch 2 -- loss : 267.9462\n",
      "Batch 101 out of 116 in Epoch 2 -- loss : 273.2548\n",
      "Batch 102 out of 116 in Epoch 2 -- loss : 254.8250\n",
      "Batch 103 out of 116 in Epoch 2 -- loss : 237.1181\n",
      "Batch 104 out of 116 in Epoch 2 -- loss : 244.4226\n",
      "Batch 105 out of 116 in Epoch 2 -- loss : 274.9168\n",
      "Batch 106 out of 116 in Epoch 2 -- loss : 252.5082\n",
      "Batch 107 out of 116 in Epoch 2 -- loss : 265.3029\n",
      "Batch 108 out of 116 in Epoch 2 -- loss : 228.5828\n",
      "Batch 109 out of 116 in Epoch 2 -- loss : 257.3998\n",
      "Batch 110 out of 116 in Epoch 2 -- loss : 268.3478\n",
      "Batch 111 out of 116 in Epoch 2 -- loss : 244.7012\n",
      "Batch 112 out of 116 in Epoch 2 -- loss : 241.0861\n",
      "Batch 113 out of 116 in Epoch 2 -- loss : 248.6102\n",
      "Batch 114 out of 116 in Epoch 2 -- loss : 255.4365\n",
      "Batch 115 out of 116 in Epoch 2 -- loss : 290.4977\n",
      "Batch 116 out of 116 in Epoch 2 -- loss : 237.4748\n",
      "Epoch 2 -- loss : 263.5104\n",
      "Batch 1 out of 116 in Epoch 3 -- loss : 258.6489\n",
      "Batch 2 out of 116 in Epoch 3 -- loss : 262.4677\n",
      "Batch 3 out of 116 in Epoch 3 -- loss : 220.3534\n",
      "Batch 4 out of 116 in Epoch 3 -- loss : 236.8216\n",
      "Batch 5 out of 116 in Epoch 3 -- loss : 247.6885\n",
      "Batch 6 out of 116 in Epoch 3 -- loss : 299.0548\n",
      "Batch 7 out of 116 in Epoch 3 -- loss : 261.1791\n",
      "Batch 8 out of 116 in Epoch 3 -- loss : 272.9197\n",
      "Batch 9 out of 116 in Epoch 3 -- loss : 262.2672\n",
      "Batch 10 out of 116 in Epoch 3 -- loss : 262.7591\n",
      "Batch 11 out of 116 in Epoch 3 -- loss : 240.0427\n",
      "Batch 12 out of 116 in Epoch 3 -- loss : 254.4376\n",
      "Batch 13 out of 116 in Epoch 3 -- loss : 236.5485\n",
      "Batch 14 out of 116 in Epoch 3 -- loss : 251.7376\n",
      "Batch 15 out of 116 in Epoch 3 -- loss : 271.1168\n",
      "Batch 16 out of 116 in Epoch 3 -- loss : 256.5273\n",
      "Batch 17 out of 116 in Epoch 3 -- loss : 243.3992\n",
      "Batch 18 out of 116 in Epoch 3 -- loss : 279.6731\n",
      "Batch 19 out of 116 in Epoch 3 -- loss : 255.0517\n",
      "Batch 20 out of 116 in Epoch 3 -- loss : 270.9019\n",
      "Batch 21 out of 116 in Epoch 3 -- loss : 252.7365\n",
      "Batch 22 out of 116 in Epoch 3 -- loss : 275.2563\n",
      "Batch 23 out of 116 in Epoch 3 -- loss : 270.2524\n",
      "Batch 24 out of 116 in Epoch 3 -- loss : 240.4103\n",
      "Batch 25 out of 116 in Epoch 3 -- loss : 232.3234\n",
      "Batch 26 out of 116 in Epoch 3 -- loss : 255.5055\n",
      "Batch 27 out of 116 in Epoch 3 -- loss : 243.9938\n",
      "Batch 28 out of 116 in Epoch 3 -- loss : 225.9791\n",
      "Batch 29 out of 116 in Epoch 3 -- loss : 237.4676\n",
      "Batch 30 out of 116 in Epoch 3 -- loss : 234.2394\n",
      "Batch 31 out of 116 in Epoch 3 -- loss : 261.9715\n",
      "Batch 32 out of 116 in Epoch 3 -- loss : 261.2926\n",
      "Batch 33 out of 116 in Epoch 3 -- loss : 237.1195\n",
      "Batch 34 out of 116 in Epoch 3 -- loss : 254.7489\n",
      "Batch 35 out of 116 in Epoch 3 -- loss : 240.5774\n",
      "Batch 36 out of 116 in Epoch 3 -- loss : 262.6199\n",
      "Batch 37 out of 116 in Epoch 3 -- loss : 245.5887\n",
      "Batch 38 out of 116 in Epoch 3 -- loss : 232.6407\n",
      "Batch 39 out of 116 in Epoch 3 -- loss : 234.8474\n",
      "Batch 40 out of 116 in Epoch 3 -- loss : 220.0489\n",
      "Batch 41 out of 116 in Epoch 3 -- loss : 231.0928\n",
      "Batch 42 out of 116 in Epoch 3 -- loss : 255.4195\n",
      "Batch 43 out of 116 in Epoch 3 -- loss : 237.8515\n",
      "Batch 44 out of 116 in Epoch 3 -- loss : 260.1417\n",
      "Batch 45 out of 116 in Epoch 3 -- loss : 249.4441\n",
      "Batch 46 out of 116 in Epoch 3 -- loss : 241.7747\n",
      "Batch 47 out of 116 in Epoch 3 -- loss : 249.9871\n",
      "Batch 48 out of 116 in Epoch 3 -- loss : 248.7772\n",
      "Batch 49 out of 116 in Epoch 3 -- loss : 238.8250\n",
      "Batch 50 out of 116 in Epoch 3 -- loss : 243.2692\n",
      "Batch 51 out of 116 in Epoch 3 -- loss : 222.4725\n",
      "Batch 52 out of 116 in Epoch 3 -- loss : 240.9637\n",
      "Batch 53 out of 116 in Epoch 3 -- loss : 256.2916\n",
      "Batch 54 out of 116 in Epoch 3 -- loss : 235.7166\n",
      "Batch 55 out of 116 in Epoch 3 -- loss : 244.1778\n",
      "Batch 56 out of 116 in Epoch 3 -- loss : 259.7187\n",
      "Batch 57 out of 116 in Epoch 3 -- loss : 265.5569\n",
      "Batch 58 out of 116 in Epoch 3 -- loss : 248.1826\n",
      "Batch 59 out of 116 in Epoch 3 -- loss : 246.6515\n",
      "Batch 60 out of 116 in Epoch 3 -- loss : 255.8320\n",
      "Batch 61 out of 116 in Epoch 3 -- loss : 254.4216\n",
      "Batch 62 out of 116 in Epoch 3 -- loss : 247.9115\n",
      "Batch 63 out of 116 in Epoch 3 -- loss : 232.4400\n",
      "Batch 64 out of 116 in Epoch 3 -- loss : 251.7480\n",
      "Batch 65 out of 116 in Epoch 3 -- loss : 258.1158\n",
      "Batch 66 out of 116 in Epoch 3 -- loss : 247.8713\n",
      "Batch 67 out of 116 in Epoch 3 -- loss : 244.1261\n",
      "Batch 68 out of 116 in Epoch 3 -- loss : 236.9230\n",
      "Batch 69 out of 116 in Epoch 3 -- loss : 253.0602\n",
      "Batch 70 out of 116 in Epoch 3 -- loss : 238.8187\n",
      "Batch 71 out of 116 in Epoch 3 -- loss : 255.7838\n",
      "Batch 72 out of 116 in Epoch 3 -- loss : 245.1222\n",
      "Batch 73 out of 116 in Epoch 3 -- loss : 243.5567\n",
      "Batch 74 out of 116 in Epoch 3 -- loss : 232.7967\n",
      "Batch 75 out of 116 in Epoch 3 -- loss : 249.8259\n",
      "Batch 76 out of 116 in Epoch 3 -- loss : 250.1236\n",
      "Batch 77 out of 116 in Epoch 3 -- loss : 228.2207\n",
      "Batch 78 out of 116 in Epoch 3 -- loss : 238.0877\n",
      "Batch 79 out of 116 in Epoch 3 -- loss : 244.2222\n",
      "Batch 80 out of 116 in Epoch 3 -- loss : 255.9885\n",
      "Batch 81 out of 116 in Epoch 3 -- loss : 247.6049\n",
      "Batch 82 out of 116 in Epoch 3 -- loss : 249.3002\n",
      "Batch 83 out of 116 in Epoch 3 -- loss : 250.8499\n",
      "Batch 84 out of 116 in Epoch 3 -- loss : 254.2158\n",
      "Batch 85 out of 116 in Epoch 3 -- loss : 248.7706\n",
      "Batch 86 out of 116 in Epoch 3 -- loss : 256.7283\n",
      "Batch 87 out of 116 in Epoch 3 -- loss : 250.8446\n",
      "Batch 88 out of 116 in Epoch 3 -- loss : 233.9049\n",
      "Batch 89 out of 116 in Epoch 3 -- loss : 264.6854\n",
      "Batch 90 out of 116 in Epoch 3 -- loss : 252.7606\n",
      "Batch 91 out of 116 in Epoch 3 -- loss : 215.5910\n",
      "Batch 92 out of 116 in Epoch 3 -- loss : 221.0454\n",
      "Batch 93 out of 116 in Epoch 3 -- loss : 223.2807\n",
      "Batch 94 out of 116 in Epoch 3 -- loss : 246.9027\n",
      "Batch 95 out of 116 in Epoch 3 -- loss : 234.7103\n",
      "Batch 96 out of 116 in Epoch 3 -- loss : 231.1466\n",
      "Batch 97 out of 116 in Epoch 3 -- loss : 229.6671\n",
      "Batch 98 out of 116 in Epoch 3 -- loss : 243.9704\n",
      "Batch 99 out of 116 in Epoch 3 -- loss : 228.4501\n",
      "Batch 100 out of 116 in Epoch 3 -- loss : 253.6287\n",
      "Batch 101 out of 116 in Epoch 3 -- loss : 255.1201\n",
      "Batch 102 out of 116 in Epoch 3 -- loss : 238.4417\n",
      "Batch 103 out of 116 in Epoch 3 -- loss : 225.7140\n",
      "Batch 104 out of 116 in Epoch 3 -- loss : 239.9986\n",
      "Batch 105 out of 116 in Epoch 3 -- loss : 265.5852\n",
      "Batch 106 out of 116 in Epoch 3 -- loss : 240.9528\n",
      "Batch 107 out of 116 in Epoch 3 -- loss : 251.5062\n",
      "Batch 108 out of 116 in Epoch 3 -- loss : 222.2856\n",
      "Batch 109 out of 116 in Epoch 3 -- loss : 247.8244\n",
      "Batch 110 out of 116 in Epoch 3 -- loss : 251.2172\n",
      "Batch 111 out of 116 in Epoch 3 -- loss : 239.0912\n",
      "Batch 112 out of 116 in Epoch 3 -- loss : 230.2752\n",
      "Batch 113 out of 116 in Epoch 3 -- loss : 236.5711\n",
      "Batch 114 out of 116 in Epoch 3 -- loss : 241.6812\n",
      "Batch 115 out of 116 in Epoch 3 -- loss : 276.8051\n",
      "Batch 116 out of 116 in Epoch 3 -- loss : 227.1602\n",
      "Epoch 3 -- loss : 246.8346\n",
      "Batch 1 out of 116 in Epoch 4 -- loss : 245.7706\n",
      "Batch 2 out of 116 in Epoch 4 -- loss : 247.8376\n",
      "Batch 3 out of 116 in Epoch 4 -- loss : 212.8788\n",
      "Batch 4 out of 116 in Epoch 4 -- loss : 228.2967\n",
      "Batch 5 out of 116 in Epoch 4 -- loss : 237.5741\n",
      "Batch 6 out of 116 in Epoch 4 -- loss : 285.3154\n",
      "Batch 7 out of 116 in Epoch 4 -- loss : 253.1969\n",
      "Batch 8 out of 116 in Epoch 4 -- loss : 260.8234\n",
      "Batch 9 out of 116 in Epoch 4 -- loss : 255.6577\n",
      "Batch 10 out of 116 in Epoch 4 -- loss : 256.2998\n",
      "Batch 11 out of 116 in Epoch 4 -- loss : 230.9051\n",
      "Batch 12 out of 116 in Epoch 4 -- loss : 243.0279\n",
      "Batch 13 out of 116 in Epoch 4 -- loss : 227.7249\n",
      "Batch 14 out of 116 in Epoch 4 -- loss : 242.4240\n",
      "Batch 15 out of 116 in Epoch 4 -- loss : 253.8343\n",
      "Batch 16 out of 116 in Epoch 4 -- loss : 247.5359\n",
      "Batch 17 out of 116 in Epoch 4 -- loss : 238.8014\n",
      "Batch 18 out of 116 in Epoch 4 -- loss : 268.7277\n",
      "Batch 19 out of 116 in Epoch 4 -- loss : 247.9659\n",
      "Batch 20 out of 116 in Epoch 4 -- loss : 258.9275\n",
      "Batch 21 out of 116 in Epoch 4 -- loss : 242.6191\n",
      "Batch 22 out of 116 in Epoch 4 -- loss : 261.7417\n",
      "Batch 23 out of 116 in Epoch 4 -- loss : 263.3581\n",
      "Batch 24 out of 116 in Epoch 4 -- loss : 232.8797\n",
      "Batch 25 out of 116 in Epoch 4 -- loss : 225.7509\n",
      "Batch 26 out of 116 in Epoch 4 -- loss : 247.5914\n",
      "Batch 27 out of 116 in Epoch 4 -- loss : 236.3021\n",
      "Batch 28 out of 116 in Epoch 4 -- loss : 215.6606\n",
      "Batch 29 out of 116 in Epoch 4 -- loss : 233.1413\n",
      "Batch 30 out of 116 in Epoch 4 -- loss : 227.2415\n",
      "Batch 31 out of 116 in Epoch 4 -- loss : 252.0639\n",
      "Batch 32 out of 116 in Epoch 4 -- loss : 248.3427\n",
      "Batch 33 out of 116 in Epoch 4 -- loss : 227.0043\n",
      "Batch 34 out of 116 in Epoch 4 -- loss : 244.7550\n",
      "Batch 35 out of 116 in Epoch 4 -- loss : 234.1624\n",
      "Batch 36 out of 116 in Epoch 4 -- loss : 253.3146\n",
      "Batch 37 out of 116 in Epoch 4 -- loss : 239.3546\n",
      "Batch 38 out of 116 in Epoch 4 -- loss : 226.7464\n",
      "Batch 39 out of 116 in Epoch 4 -- loss : 228.6821\n",
      "Batch 40 out of 116 in Epoch 4 -- loss : 213.6343\n",
      "Batch 41 out of 116 in Epoch 4 -- loss : 226.7661\n",
      "Batch 42 out of 116 in Epoch 4 -- loss : 242.7333\n",
      "Batch 43 out of 116 in Epoch 4 -- loss : 229.5889\n",
      "Batch 44 out of 116 in Epoch 4 -- loss : 249.3324\n",
      "Batch 45 out of 116 in Epoch 4 -- loss : 242.5979\n",
      "Batch 46 out of 116 in Epoch 4 -- loss : 232.3665\n",
      "Batch 47 out of 116 in Epoch 4 -- loss : 242.4117\n",
      "Batch 48 out of 116 in Epoch 4 -- loss : 239.8335\n",
      "Batch 49 out of 116 in Epoch 4 -- loss : 233.5146\n",
      "Batch 50 out of 116 in Epoch 4 -- loss : 235.4103\n",
      "Batch 51 out of 116 in Epoch 4 -- loss : 217.2370\n",
      "Batch 52 out of 116 in Epoch 4 -- loss : 232.8848\n",
      "Batch 53 out of 116 in Epoch 4 -- loss : 248.2532\n",
      "Batch 54 out of 116 in Epoch 4 -- loss : 227.8739\n",
      "Batch 55 out of 116 in Epoch 4 -- loss : 237.3745\n",
      "Batch 56 out of 116 in Epoch 4 -- loss : 250.7653\n",
      "Batch 57 out of 116 in Epoch 4 -- loss : 259.3657\n",
      "Batch 58 out of 116 in Epoch 4 -- loss : 237.8250\n",
      "Batch 59 out of 116 in Epoch 4 -- loss : 235.0559\n",
      "Batch 60 out of 116 in Epoch 4 -- loss : 247.7548\n",
      "Batch 61 out of 116 in Epoch 4 -- loss : 247.9101\n",
      "Batch 62 out of 116 in Epoch 4 -- loss : 237.7596\n",
      "Batch 63 out of 116 in Epoch 4 -- loss : 221.9557\n",
      "Batch 64 out of 116 in Epoch 4 -- loss : 242.5220\n",
      "Batch 65 out of 116 in Epoch 4 -- loss : 252.3121\n",
      "Batch 66 out of 116 in Epoch 4 -- loss : 244.6481\n",
      "Batch 67 out of 116 in Epoch 4 -- loss : 236.2312\n",
      "Batch 68 out of 116 in Epoch 4 -- loss : 230.8635\n",
      "Batch 69 out of 116 in Epoch 4 -- loss : 245.6554\n",
      "Batch 70 out of 116 in Epoch 4 -- loss : 229.0279\n",
      "Batch 71 out of 116 in Epoch 4 -- loss : 243.2883\n",
      "Batch 72 out of 116 in Epoch 4 -- loss : 234.9417\n",
      "Batch 73 out of 116 in Epoch 4 -- loss : 233.2918\n",
      "Batch 74 out of 116 in Epoch 4 -- loss : 226.8754\n",
      "Batch 75 out of 116 in Epoch 4 -- loss : 243.1649\n",
      "Batch 76 out of 116 in Epoch 4 -- loss : 244.7756\n",
      "Batch 77 out of 116 in Epoch 4 -- loss : 221.4766\n",
      "Batch 78 out of 116 in Epoch 4 -- loss : 232.2690\n",
      "Batch 79 out of 116 in Epoch 4 -- loss : 234.8830\n",
      "Batch 80 out of 116 in Epoch 4 -- loss : 245.0187\n",
      "Batch 81 out of 116 in Epoch 4 -- loss : 241.4633\n",
      "Batch 82 out of 116 in Epoch 4 -- loss : 241.6394\n",
      "Batch 83 out of 116 in Epoch 4 -- loss : 242.6532\n",
      "Batch 84 out of 116 in Epoch 4 -- loss : 247.3472\n",
      "Batch 85 out of 116 in Epoch 4 -- loss : 245.1142\n",
      "Batch 86 out of 116 in Epoch 4 -- loss : 246.7953\n",
      "Batch 87 out of 116 in Epoch 4 -- loss : 246.0509\n",
      "Batch 88 out of 116 in Epoch 4 -- loss : 226.8115\n",
      "Batch 89 out of 116 in Epoch 4 -- loss : 256.7404\n",
      "Batch 90 out of 116 in Epoch 4 -- loss : 243.6955\n",
      "Batch 91 out of 116 in Epoch 4 -- loss : 212.1694\n",
      "Batch 92 out of 116 in Epoch 4 -- loss : 219.0867\n",
      "Batch 93 out of 116 in Epoch 4 -- loss : 219.4068\n",
      "Batch 94 out of 116 in Epoch 4 -- loss : 240.1318\n",
      "Batch 95 out of 116 in Epoch 4 -- loss : 229.2791\n",
      "Batch 96 out of 116 in Epoch 4 -- loss : 225.0485\n",
      "Batch 97 out of 116 in Epoch 4 -- loss : 222.3454\n",
      "Batch 98 out of 116 in Epoch 4 -- loss : 236.8396\n",
      "Batch 99 out of 116 in Epoch 4 -- loss : 225.3468\n",
      "Batch 100 out of 116 in Epoch 4 -- loss : 244.8877\n",
      "Batch 101 out of 116 in Epoch 4 -- loss : 250.7998\n",
      "Batch 102 out of 116 in Epoch 4 -- loss : 232.2690\n",
      "Batch 103 out of 116 in Epoch 4 -- loss : 223.1076\n",
      "Batch 104 out of 116 in Epoch 4 -- loss : 236.4719\n",
      "Batch 105 out of 116 in Epoch 4 -- loss : 257.3429\n",
      "Batch 106 out of 116 in Epoch 4 -- loss : 233.1267\n",
      "Batch 107 out of 116 in Epoch 4 -- loss : 241.5986\n",
      "Batch 108 out of 116 in Epoch 4 -- loss : 222.6591\n",
      "Batch 109 out of 116 in Epoch 4 -- loss : 240.1525\n",
      "Batch 110 out of 116 in Epoch 4 -- loss : 244.3090\n",
      "Batch 111 out of 116 in Epoch 4 -- loss : 233.5713\n",
      "Batch 112 out of 116 in Epoch 4 -- loss : 224.0785\n",
      "Batch 113 out of 116 in Epoch 4 -- loss : 226.2084\n",
      "Batch 114 out of 116 in Epoch 4 -- loss : 232.9117\n",
      "Batch 115 out of 116 in Epoch 4 -- loss : 271.1498\n",
      "Batch 116 out of 116 in Epoch 4 -- loss : 221.2590\n",
      "Epoch 4 -- loss : 238.9617\n"
     ]
    }
   ],
   "source": [
    "# Set-up training\n",
    "nb_of_epochs = nb_iteration\n",
    "batch_size = lot_size  # The backpropagation is done after every batch, but a batch here is also used for memory requirements\n",
    "number_of_batches = len(train_indices) // batch_size\n",
    "\n",
    "params = [w1, w2]  # Trainable parameters\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "for e in range(nb_of_epochs):\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "    for batch in np.array_split(train_indices, number_of_batches):\n",
    "        i += 1\n",
    "        # Select batch and convert to tensors\n",
    "        batch_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "        batch_labels = torch.LongTensor(y[batch, np.newaxis]).to(device)\n",
    "\n",
    "        # Here we create a target spike count (10 spikes for wrong label, 100 spikes for true label) in a one-hot fashion\n",
    "        # This approach is seen in Shrestha & Orchard (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "        # Code available at https://github.com/bamsumit/slayerPytorch\n",
    "        min_spike_count = 10 * torch.ones((batch.shape[0], 10), device=device, dtype=torch.float)\n",
    "        target_output = min_spike_count.scatter_(1, batch_labels, 100.0)\n",
    "\n",
    "        # Forward propagation\n",
    "        layer_1_spikes = run_spiking_layer(batch_spike_train, w1)\n",
    "        layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "        network_output = torch.sum(layer_2_spikes, 2)  # Count the spikes over time axis\n",
    "        loss = loss_fn(network_output, target_output)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print(\"Batch %i out of %i in Epoch %i -- loss : %.4f\" %(i, number_of_batches, e+1, loss.item()))\n",
    "    \n",
    "    print(\"Epoch %i -- loss : %.4f\" %(e+1, epoch_loss / number_of_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j4qQrgVnihY"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NgQyFnPetmxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy of the model\n",
    "correct_label_count = 0\n",
    "\n",
    "# We only need to batchify the test set for memory requirements\n",
    "for batch in np.array_split(test_indices,  len(test_indices) // batch_size):\n",
    "    test_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "\n",
    "    # Same forward propagation as before\n",
    "    layer_1_spikes = run_spiking_layer(test_spike_train, w1)\n",
    "    layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "    network_output = torch.sum(layer_2_spikes, 2)  # Count the spikes over time axis\n",
    "\n",
    "    # Do the prediction by selecting the output neuron with the most number of spikes\n",
    "    _, am = torch.max(network_output, 1)\n",
    "    correct_label_count += np.sum(am.detach().cpu().numpy() == y[batch])\n",
    "\n",
    "print(\"Model accuracy on test set: %.3f\" % (correct_label_count / len(test_indices)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
